{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dtale as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, cross_val_predict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "\n",
    "The train and test were provided separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the D-tale library to display the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.show(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.show(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Data Cleaning and Analysis\n",
    "<!-- #### 1. Understanding meaning of each column: -->\n",
    "<!-- <br>Data Dictionary: -->\n",
    "<br>**Variable        Description**</br>\n",
    "1. Survived\t- Survived (1) or died (0)\n",
    "2. Pclass -\tPassenger’s class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "3. Name\t- Passenger’s name\n",
    "4. Sex -\tPassenger’s sex\n",
    "5. Age\t- Passenger’s age\n",
    "6. SibSp -\tNumber of siblings/spouses aboard\n",
    "7. Parch -\tNumber of parents/children aboard (Some children travelled only with a nanny, therefore parch=0 for them.)\n",
    "8. Ticket -\tTicket number\n",
    "9. Fare -\tFare\n",
    "10. Cabin -\tCabin\n",
    "11. Embarked -\tPort of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "From the above, it was discovered that;\n",
    "\n",
    "1. The train dataset was (891 rows, 12 column) and the test (418 rows, 11 collumns)\n",
    "2. There are missing value in age, Embarked and cabin for both train and test dataset\n",
    "3. Some columns are not needed to determining the survival of a person e.g. Name, PassengerId, Ticket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting unnecessary column for both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deleting the columns not required for determining the survival of a person in the train data\n",
    "\"\"\"\n",
    "\n",
    "del train['PassengerId']\n",
    "del train['Ticket']\n",
    "del train['Fare']\n",
    "del train['Cabin']\n",
    "del train['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similarly, deleting the columns not required for determining the survival of a person for the test.csv data\n",
    "\"\"\"\n",
    "\n",
    "del test['Ticket']\n",
    "del test['Fare']\n",
    "del test['Cabin']\n",
    "del test['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation method for missing value\n",
    "\n",
    "The cabin columns has been removed since it is not usefull.\n",
    "\n",
    "#### Embarked column has 2 missing value in the train data, though removing or doing an imputation will not make much of a difference, Imputation was chosen. The missing value will be replaced by the mode since it is an object data type.\n",
    "\n",
    "\n",
    "#### Age column has missing value for some of the people in both training and testing data. It can be solved by \n",
    "* filling the ones who have survived with the mean age of the survived people\n",
    "* similarly fill those who haven't survived with the mean age of all non-survived people.\n",
    "\n",
    "#### But Note, this type of imputation will not be proper for testing, this is because the null will be filled with the single value of mean to predict their survival status but this will not generalise as the case may be. \n",
    "\n",
    "* To solve the issue, an array of random numbers which are generated from mean age value in regards to standard deviation and is_null will be used for the missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a list of training and testing dataset\n",
    "\n",
    "titanic_data = [train, test]\n",
    "\n",
    "for data in titanic_data:\n",
    "    mean = train['Age'].mean()\n",
    "    std = test['Age'].std()\n",
    "    is_null = data[\"Age\"].isnull().sum()\n",
    "    \n",
    "    # random numbers from mean, standard deviation and is_null will be computed\n",
    "    random_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = data[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = random_age\n",
    "    data['Age'] = age_slice\n",
    "    data['Age'] = train['Age'].astype(int)\n",
    "    \n",
    "train['Age'].isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'].fillna((train['Embarked'].value_counts().index[0]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding\n",
    "\n",
    "#### Column sex and Embarked neede to be encoded as they are categorical feature.\n",
    "* One-Hot Encoding will be used in encoding the sex feature\n",
    "* Label encoding will be used for Embarked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mark the variable as categorical type\n",
    "\n",
    "train['Sex'] = train['Sex'].astype('category')\n",
    "test['Sex'] = test['Sex'].astype('category')\n",
    "\n",
    "train['Embarked'] = train['Embarked'].astype('category')\n",
    "test['Embarked'] = test['Embarked'].astype('category')\n",
    "\n",
    "# Also convert the passenger's class to category\n",
    "# train['Pclass'] = train['Pclass'].astype('category')\n",
    "# test['Pclass'] = test['Pclass'].astype('category')\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sex'] = pd.get_dummies(train['Sex'])\n",
    "test[\"Sex\"] = pd.get_dummies(test['Sex'])\n",
    "\n",
    "train['Embarked'] = train['Embarked'].cat.codes\n",
    "test['Embarked'] = test['Embarked'].cat.codes\n",
    "\n",
    "# train['Sex'] = train['Sex'].cat.codes\n",
    "# test['Sex'] = test['Sex'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Data by Visualization\n",
    "\n",
    "Inorder to understand who would have had a better probability of survival, we should visualize the patients who survived based on age, passenger class and etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(train['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Age\n",
    "\n",
    "age_hist = sns.FacetGrid(train, col='Survived')\n",
    "age_hist.map(plt.hist, 'Age')\n",
    "age_hist.set_ylabels('Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age 20-40 years are among those that didn't survived and the infants have higher number of survised than the teenager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Passenger class(Pclass) and Age to determine who survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_age_grid = sns.FacetGrid(train, col='Survived', row='Pclass', height=2.0, aspect=1.6)\n",
    "pclass_age_grid.map(plt.hist, 'Age', alpha=0.5, bins=20)\n",
    "pclass_age_grid.add_legend()\n",
    "pclass_age_grid.set_ylabels('Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People in the 3rd class with age range of 20 - 40 years are those that didn't survived compare to others most especially the 1st Pclass which have the most survived people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining 'Parch' & 'SibSp' as 'Realtives'\n",
    "train['Family_Members']=train['Parch']+train['SibSp'] + 1\n",
    "test['Family_Members']=test['Parch']+test['SibSp'] + 1\n",
    "\n",
    "del train['SibSp']\n",
    "del train['Parch']\n",
    "\n",
    "del test['SibSp']\n",
    "del test['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Groupping\n",
    "\n",
    "For better prediction, the Age will be groupped to sub-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data=[train,test]\n",
    "\n",
    "for data in titanic_data:\n",
    "    data.loc[ data['Age'] <= 10, 'Age'] = 0\n",
    "    data.loc[(data['Age'] > 10) & (data['Age'] <= 20), 'Age'] = 1\n",
    "    data.loc[(data['Age'] > 20) & (data['Age'] <= 35), 'Age'] = 2\n",
    "    data.loc[(data['Age'] > 35) & (data['Age'] <= 45), 'Age'] = 3\n",
    "    data.loc[ data['Age'] > 45 , 'Age'] = 4\n",
    "\n",
    "train['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 11))\n",
    "sns.heatmap(train.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(test.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both the training and testing dataset do not seem to have any correllated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.show(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.show(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model for Prediction\n",
    "\n",
    "#### Since the survival is represented as either 0 or 1, therefore it is a Classification problem. The algorimths will be using are:\n",
    "\n",
    "* #### Logistic Regression\n",
    "* #### Support Vector Machines\n",
    "* #### KNN or K-Nearest Neighbors\n",
    "* #### Decision Trees\n",
    "* #### Random Forest\n",
    "* #### Stochastic Gradient descent (SGD)\n",
    "* #### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin with, let drop and assign the survival columns from train and drop passengerID from test\n",
    "\n",
    "X_train= train.drop(['Survived'], axis =1)\n",
    "y_train= train['Survived']\n",
    "\n",
    "X_test=test.drop('PassengerId', axis=1).copy()\n",
    "\n",
    "print('X_train: {}\\nX_test: {}\\ny_train: {}'.format(X_train.shape, X_test.shape, y_train.shape ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test_std)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of the linear regressor model\n",
    "\n",
    "log_reg_acc = round(log_reg.score(X_train_std,y_train)*100, 2)\n",
    "print(log_reg_acc,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(y_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of the linear regressor model\n",
    "\n",
    "log_reg_acc = round(log_reg.score(X_train,y_train)*100, 2)\n",
    "print(log_reg_acc,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_coeff= pd.DataFrame(train.columns.delete(0))\n",
    "# df_coeff.columns = ['Feature']\n",
    "# df_coeff['Correlation'] = pd.Series(log_reg.coef_[0])\n",
    "\n",
    "# df_coeff.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "#### Cross-Validation protects against overfitting.\n",
    "#### It is a resampling method which tells us how well our model would generalize to unseen data. This is achieved by fixing a number of partitions of the dataset called folds, predicting each fold separately, and averaging the predictions in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our robust cross-validation scheme!\n",
    "kf = KFold(n_splits = 10, random_state = 2)\n",
    "\n",
    "# Print CV accuracy estimate:\n",
    "#print(cross_val_score(logisticRegression, X_test, y_test, cv = kf).mean())\n",
    "scores= cross_val_score(log_reg, X_train, y_train, cv = kf, scoring='accuracy')\n",
    "\n",
    "mean_acc_log = scores.mean()*100\n",
    "\n",
    "print('Scores: ', scores*100, '%')\n",
    "print('Mean: {0:.2f}%'.format(mean_acc_log))\n",
    "print('Standard Deviation: ', scores.std()*100, '%\\n')\n",
    "\n",
    "pred= cross_val_predict(log_reg, X_train, y_train, cv=kf)\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train, pred),'\\n')\n",
    "\n",
    "print(\"Precision: \", round(precision_score(y_train, pred)*100, 2),'%')\n",
    "print(\"Recall: \", round(recall_score(y_train, pred)*100, 2), '%')\n",
    "print('F1 Score: ', round(f1_score(y_train, pred)*100, 2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first row is about the not-survived-predictions: 470 passengers were correctly classified as not survived (called true negatives) and 79 where wrongly classified as not survived (false positives).\n",
    "### The second row is about the survived-predictions: 110 passengers where wrongly classified as survived (false negatives) and 230 where correctly classified as survived (true positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of the support vector model\n",
    "\n",
    "svc_acc = round(svc.score(X_train_std,y_train)*100, 2)\n",
    "print(svc_acc,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print CV accuracy estimate:\n",
    "#print(cross_val_score(SVC(), X_test, y_test, cv = kf).mean())\n",
    "\n",
    "scores= cross_val_score(SVC(), X_train, y_train, cv = kf, scoring='accuracy')\n",
    "\n",
    "mean_acc_svc = scores.mean()*100\n",
    "\n",
    "print('Scores: ', scores*100, '%')\n",
    "print('Mean: ', mean_acc_svc, '%')\n",
    "print('Standard Deviation: ', scores.std()*100, '%\\n')\n",
    "\n",
    "pred= cross_val_predict(svc, X_train, y_train, cv=kf)\n",
    "print('Confusion Matrix: \\n' ,confusion_matrix(y_train, pred),'\\n')\n",
    "\n",
    "print(\"Precision: \", round(precision_score(y_train, pred)*100, 2),'%')\n",
    "print(\"Recall: \", round(recall_score(y_train, pred)*100, 2), '%')\n",
    "print('F1 Score: ', round(f1_score(y_train, pred)*100, 2), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':(0.001,0.005,0.01,0.05, 0.1, 0.5, 1, 5, 10, 50,100,500,1000)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_l = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_lin = GridSearchCV(clf_svm_l, params, n_jobs=-1,\n",
    "                            cv=10, verbose=1, scoring='accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_lin.fit(X_train_std, y_train)\n",
    "svm_grid_lin.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvm_clf = svm_grid_lin.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_acc=round(svm_grid_lin.score(X_train_std,y_train)*100, 2)\n",
    "print(svc_acc,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax =sns.heatmap(cm, square=True, annot=True, cbar=False, fmt='d')\n",
    "ax.xaxis.set_ticklabels(class_names, fontsize = 12)\n",
    "ax.yaxis.set_ticklabels(class_names, fontsize = 12, rotation=0)\n",
    "ax.set_xlabel('Predicted Labels',fontsize = 15)\n",
    "ax.set_ylabel('True Labels',fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
